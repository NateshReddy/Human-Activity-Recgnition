{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWTFZpoe78zy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and wrote 76 video files.\n"
     ]
    }
   ],
   "source": [
    "#EXTRACTING FRAMES\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import os.path\n",
    "from subprocess import call\n",
    "import os\n",
    "\n",
    "def extract_files():\n",
    "    data_file = []\n",
    "    folders = ['train','test']\n",
    "    for folder in folders:\n",
    "        class_folders = glob.glob(os.path.join(folder, '*'))\n",
    "\n",
    "        for vid_class in class_folders:\n",
    "            class_files = glob.glob(os.path.join(vid_class, '*.avi'))\n",
    "\n",
    "            for video_path in class_files:\n",
    "                # Get the parts of the file.\n",
    "                video_parts = get_video_parts(video_path)\n",
    "\n",
    "                train_or_test, classname, filename_no_ext, filename = video_parts\n",
    "\n",
    "                # Only extract if we haven't done it yet. Otherwise, just get\n",
    "                # the info.\n",
    "                if not check_already_extracted(video_parts):\n",
    "                    # Now extract it.\n",
    "                    src = os.path.join(train_or_test, classname, filename)\n",
    "                    dest = os.path.join(train_or_test, classname,\n",
    "                        filename_no_ext + '-%04d.jpg')\n",
    "                    call([\"ffmpeg\", \"-i\", src, dest])\n",
    "\n",
    "                # Now get how many frames it is.\n",
    "                nb_frames = get_nb_frames_for_video(video_parts)\n",
    "\n",
    "                data_file.append([train_or_test, classname, filename_no_ext, nb_frames])\n",
    "\n",
    "                #print(\"Generated %d frames for %s\" % (nb_frames, filename_no_ext))\n",
    "\n",
    "    with open('data_file.csv', 'w') as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerows(data_file)\n",
    "    #[train|test], class, filename, nb frames\n",
    "    print(\"Extracted and wrote %d video files.\" % (len(data_file)))\n",
    "\n",
    "def get_nb_frames_for_video(video_parts):\n",
    "    train_or_test, classname, filename_no_ext, _ = video_parts\n",
    "    generated_files = glob.glob(os.path.join(train_or_test, classname,\n",
    "                                filename_no_ext + '*.jpg'))\n",
    "    return len(generated_files)\n",
    "\n",
    "def get_video_parts(video_path):\n",
    "    parts = video_path.split(os.path.sep)\n",
    "    filename = parts[2]\n",
    "    filename_no_ext = filename.split('.')[0]\n",
    "    classname = parts[1]\n",
    "    train_or_test = parts[0]\n",
    "\n",
    "    return train_or_test, classname, filename_no_ext, filename\n",
    "\n",
    "def check_already_extracted(video_parts):\n",
    "    train_or_test, classname, filename_no_ext, _ = video_parts\n",
    "    return bool(os.path.exists(os.path.join(train_or_test, classname,\n",
    "                               filename_no_ext + '-0001.jpg')))\n",
    "\n",
    "extract_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cf64XB5A9prf"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import operator\n",
    "import threading\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "class DataSet():\n",
    "\n",
    "    def __init__(self, seq_length=40, class_limit=None, image_shape=(224, 224, 3)):\n",
    "        self.seq_length = seq_length\n",
    "        self.class_limit = class_limit\n",
    "        self.sequence_path = os.path.join('data', 'sequences')\n",
    "        self.max_frames = 300  # max number of frames a video can have for us to use it\n",
    "        self.data = self.get_data()\n",
    "        self.classes = self.get_classes()\n",
    "        self.data = self.clean_data()\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        with open(os.path.join('data', 'data_file.csv'), 'r') as fin:\n",
    "            reader = csv.reader(fin)\n",
    "            data = list(reader)\n",
    "        return data\n",
    "\n",
    "    def clean_data(self):\n",
    "        data_clean = []\n",
    "        for item in self.data:\n",
    "            if int(item[3]) >= self.seq_length and int(item[3]) <= self.max_frames \\\n",
    "                    and item[1] in self.classes:\n",
    "                data_clean.append(item)\n",
    "\n",
    "        return data_clean\n",
    "\n",
    "    def get_classes(self):\n",
    "        classes = []\n",
    "        for item in self.data:\n",
    "            if item[1] not in classes:\n",
    "                classes.append(item[1])\n",
    "        classes = sorted(classes)\n",
    "        if self.class_limit is not None:\n",
    "            return classes[:self.class_limit]\n",
    "        else:\n",
    "            return classes\n",
    "\n",
    "    def get_class_one_hot(self, class_str):\n",
    "        # Encode it first.\n",
    "        label_encoded = self.classes.index(class_str)\n",
    "        # Now one-hot it.\n",
    "        label_hot = to_categorical(label_encoded, len(self.classes))\n",
    "        assert len(label_hot) == len(self.classes)\n",
    "        return label_hot\n",
    "\n",
    "    def split_train_test(self):\n",
    "        train = []\n",
    "        test = []\n",
    "        for item in self.data:\n",
    "            if item[0] == 'train':\n",
    "                train.append(item)\n",
    "            else:\n",
    "                test.append(item)\n",
    "        return train, test\n",
    "\n",
    "    def get_all_sequences_in_memory(self, train_test, data_type):\n",
    "        train, test = self.split_train_test()\n",
    "        data = train if train_test == 'train' else test\n",
    "\n",
    "        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n",
    "\n",
    "        X, y = [], []\n",
    "        for row in data:\n",
    "            sequence = self.get_extracted_sequence(data_type, row)\n",
    "            if sequence is None:\n",
    "                print(\"Can't find sequence. Did you generate them?\")\n",
    "                raise\n",
    "            X.append(sequence)\n",
    "            y.append(self.get_class_one_hot(row[1]))\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def get_extracted_sequence(self, data_type, sample):\n",
    "        filename = sample[2]\n",
    "        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n",
    "            '-' + data_type + '.npy')\n",
    "        if os.path.isfile(path):\n",
    "            return np.load(path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_frames_by_filename(self, filename, data_type):\n",
    "        sample = None\n",
    "        for row in self.data:\n",
    "            if row[2] == filename:\n",
    "                sample = row\n",
    "                break\n",
    "        if sample is None:\n",
    "            raise ValueError(\"Couldn't find sample: %s\" % filename)\n",
    "        sequence = self.get_extracted_sequence(data_type, sample)\n",
    "        if sequence is None:\n",
    "            raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
    "        return sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_for_sample(sample):\n",
    "        \"\"\"Given a sample row from the data file, get all the corresponding frame\n",
    "        filenames.\"\"\"\n",
    "        path = os.path.join('data', sample[0], sample[1])\n",
    "        filename = sample[2]\n",
    "        images = sorted(glob.glob(os.path.join(path, filename + '*jpg')))\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_list(input_list, size):\n",
    "        assert len(input_list)>= size\n",
    "        skip = len(input_list) // size\n",
    "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "        return output[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CH2FCn0X9rU4",
    "outputId": "7d888dc8-136a-49d7-c652-e3c4daafd739"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 1/34 [00:09<04:58,  9.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 2/34 [00:16<04:30,  8.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 3/34 [00:22<04:05,  7.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 4/34 [00:30<03:57,  7.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|█▍        | 5/34 [00:37<03:41,  7.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 18%|█▊        | 6/34 [00:44<03:31,  7.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 21%|██        | 7/34 [00:51<03:13,  7.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▎       | 8/34 [00:57<02:59,  6.91s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|██▋       | 9/34 [01:03<02:47,  6.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▉       | 10/34 [01:10<02:37,  6.56s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 32%|███▏      | 11/34 [01:16<02:29,  6.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 35%|███▌      | 12/34 [01:22<02:21,  6.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 13/34 [01:28<02:13,  6.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 14/34 [01:35<02:06,  6.33s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████▍     | 15/34 [01:41<01:59,  6.31s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 47%|████▋     | 16/34 [01:47<01:53,  6.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 34/34 [01:56<00:00,  3.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from keras.preprocessing import image as Img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the dataset.\n",
    "#seq_length = 40\n",
    "data = DataSet(seq_length=40, class_limit=2)\n",
    "#print(data)\n",
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=True\n",
    ")\n",
    "# We'll extract features at the final pool layer.\n",
    "model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.get_layer('avg_pool').output\n",
    ")\n",
    "\n",
    "# Loop through data.\n",
    "pbar = tqdm(total=len(data.data))\n",
    "for video in data.data:\n",
    "    # Get the path to the sequence for this video.\n",
    "    path = os.path.join('data', 'sequences', video[2] + '-' + str(seq_length) + \\\n",
    "        '-features')  # numpy will auto-append .npy\n",
    "    # Check if we already have it.\n",
    "    if os.path.isfile(path + '.npy'):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Get the frames for this video.\n",
    "    frames = data.get_frames_for_sample(video)\n",
    "    #print(frames)\n",
    "\n",
    "    # Now downsample to just the ones we need.\n",
    "    frames = data.rescale_list(frames, 40)\n",
    "    #print(frames)\n",
    "    #extracting features and appending to build the sequence.\n",
    "    sequence = []\n",
    "    for image in frames:\n",
    "        img = Img.load_img(image, target_size=(299, 299))\n",
    "        x = Img.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = model.predict(x)\n",
    "        sequence.append(features[0])\n",
    "\n",
    "    # Save the sequence.\n",
    "    np.save(path, sequence)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kNnrcf6n9v8A",
    "outputId": "45ff9fc5-b90b-49b7-85f1-40b6c298bfe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 17 samples into memory for training.\n",
      "Loading 17 samples into memory for testing.\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 2048)              33562624  \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 34,612,738\n",
      "Trainable params: 34,612,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 17 samples, validate on 17 samples\n",
      "Epoch 1/100\n",
      "17/17 [==============================] - 6s 328ms/step - loss: 0.8579 - accuracy: 0.3529 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.4706 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74458, saving model to data/checkpoints/lstm-features.001-0.745.hdf5\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 5s 293ms/step - loss: 0.8014 - accuracy: 0.4118 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.7197 - val_accuracy: 0.4706 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74458 to 0.71974, saving model to data/checkpoints/lstm-features.002-0.720.hdf5\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 5s 318ms/step - loss: 0.8199 - accuracy: 0.4118 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6968 - val_accuracy: 0.5882 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71974 to 0.69679, saving model to data/checkpoints/lstm-features.003-0.697.hdf5\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 7s 407ms/step - loss: 0.8192 - accuracy: 0.4118 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 0.6471 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69679 to 0.67611, saving model to data/checkpoints/lstm-features.004-0.676.hdf5\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 6s 379ms/step - loss: 0.8004 - accuracy: 0.4118 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.6471 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67611 to 0.65766, saving model to data/checkpoints/lstm-features.005-0.658.hdf5\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 4s 263ms/step - loss: 0.7408 - accuracy: 0.5294 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6419 - val_accuracy: 0.5294 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.65766 to 0.64188, saving model to data/checkpoints/lstm-features.006-0.642.hdf5\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 4s 255ms/step - loss: 0.7047 - accuracy: 0.4706 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 0.5294 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.64188 to 0.62741, saving model to data/checkpoints/lstm-features.007-0.627.hdf5\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.6800 - accuracy: 0.4706 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.5294 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.62741 to 0.61319, saving model to data/checkpoints/lstm-features.008-0.613.hdf5\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.7592 - accuracy: 0.4706 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5995 - val_accuracy: 0.5294 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.61319 to 0.59954, saving model to data/checkpoints/lstm-features.009-0.600.hdf5\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 5s 265ms/step - loss: 0.6910 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5882 - val_accuracy: 0.5294 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.59954 to 0.58816, saving model to data/checkpoints/lstm-features.010-0.588.hdf5\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 5s 269ms/step - loss: 0.7126 - accuracy: 0.5294 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5763 - val_accuracy: 0.5882 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.58816 to 0.57631, saving model to data/checkpoints/lstm-features.011-0.576.hdf5\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.5836 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5637 - val_accuracy: 0.5882 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.57631 to 0.56367, saving model to data/checkpoints/lstm-features.012-0.564.hdf5\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.5302 - accuracy: 0.6471 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5502 - val_accuracy: 0.6471 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.56367 to 0.55018, saving model to data/checkpoints/lstm-features.013-0.550.hdf5\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.6384 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5360 - val_accuracy: 0.7059 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.55018 to 0.53599, saving model to data/checkpoints/lstm-features.014-0.536.hdf5\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.6193 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.7647 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.53599 to 0.52130, saving model to data/checkpoints/lstm-features.015-0.521.hdf5\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 4s 254ms/step - loss: 0.6324 - accuracy: 0.6471 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8235 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.52130 to 0.50726, saving model to data/checkpoints/lstm-features.016-0.507.hdf5\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.6383 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4941 - val_accuracy: 0.8235 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.50726 to 0.49411, saving model to data/checkpoints/lstm-features.017-0.494.hdf5\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.5890 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.8235 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.49411 to 0.48102, saving model to data/checkpoints/lstm-features.018-0.481.hdf5\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.4697 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.8235 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48102 to 0.46767, saving model to data/checkpoints/lstm-features.019-0.468.hdf5\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 4s 252ms/step - loss: 0.5484 - accuracy: 0.6471 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4550 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.46767 to 0.45502, saving model to data/checkpoints/lstm-features.020-0.455.hdf5\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 5s 308ms/step - loss: 0.6287 - accuracy: 0.6471 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4425 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.45502 to 0.44247, saving model to data/checkpoints/lstm-features.021-0.442.hdf5\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.6004 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4298 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: val_loss improved from 0.44247 to 0.42978, saving model to data/checkpoints/lstm-features.022-0.430.hdf5\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 4s 251ms/step - loss: 0.5020 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4181 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.42978 to 0.41807, saving model to data/checkpoints/lstm-features.023-0.418.hdf5\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.5839 - accuracy: 0.6471 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.41807 to 0.40730, saving model to data/checkpoints/lstm-features.024-0.407.hdf5\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 4s 253ms/step - loss: 0.4375 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.40730 to 0.39696, saving model to data/checkpoints/lstm-features.025-0.397.hdf5\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 4s 252ms/step - loss: 0.5951 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3867 - val_accuracy: 0.8824 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.39696 to 0.38675, saving model to data/checkpoints/lstm-features.026-0.387.hdf5\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 4s 262ms/step - loss: 0.5103 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.9412 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.38675 to 0.37707, saving model to data/checkpoints/lstm-features.027-0.377.hdf5\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.5773 - accuracy: 0.5882 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.9412 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.37707 to 0.36764, saving model to data/checkpoints/lstm-features.028-0.368.hdf5\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 5s 314ms/step - loss: 0.4407 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3584 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.36764 to 0.35836, saving model to data/checkpoints/lstm-features.029-0.358.hdf5\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 4s 263ms/step - loss: 0.5231 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.35836 to 0.34955, saving model to data/checkpoints/lstm-features.030-0.350.hdf5\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.4029 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.34955 to 0.34092, saving model to data/checkpoints/lstm-features.031-0.341.hdf5\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 5s 294ms/step - loss: 0.4927 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.34092 to 0.33250, saving model to data/checkpoints/lstm-features.032-0.333.hdf5\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 5s 302ms/step - loss: 0.5588 - accuracy: 0.7059 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.33250 to 0.32436, saving model to data/checkpoints/lstm-features.033-0.324.hdf5\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 4s 254ms/step - loss: 0.5324 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3164 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.32436 to 0.31641, saving model to data/checkpoints/lstm-features.034-0.316.hdf5\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.4463 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.31641 to 0.30873, saving model to data/checkpoints/lstm-features.035-0.309.hdf5\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.4489 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.3010 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.30873 to 0.30097, saving model to data/checkpoints/lstm-features.036-0.301.hdf5\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.3426 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2934 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.30097 to 0.29340, saving model to data/checkpoints/lstm-features.037-0.293.hdf5\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.4322 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.29340 to 0.28614, saving model to data/checkpoints/lstm-features.038-0.286.hdf5\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 5s 303ms/step - loss: 0.4335 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2793 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.28614 to 0.27930, saving model to data/checkpoints/lstm-features.039-0.279.hdf5\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 4s 255ms/step - loss: 0.3773 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.27930 to 0.27276, saving model to data/checkpoints/lstm-features.040-0.273.hdf5\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.5285 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.27276 to 0.26657, saving model to data/checkpoints/lstm-features.041-0.267.hdf5\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 5s 307ms/step - loss: 0.4099 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2605 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.26657 to 0.26048, saving model to data/checkpoints/lstm-features.042-0.260.hdf5\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.3984 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2545 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.26048 to 0.25454, saving model to data/checkpoints/lstm-features.043-0.255.hdf5\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 4s 262ms/step - loss: 0.3727 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.25454 to 0.24847, saving model to data/checkpoints/lstm-features.044-0.248.hdf5\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 5s 304ms/step - loss: 0.3633 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.24847 to 0.24255, saving model to data/checkpoints/lstm-features.045-0.243.hdf5\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 4s 258ms/step - loss: 0.3989 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2367 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.24255 to 0.23667, saving model to data/checkpoints/lstm-features.046-0.237.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.3391 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.23667 to 0.23080, saving model to data/checkpoints/lstm-features.047-0.231.hdf5\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.3416 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.23080 to 0.22509, saving model to data/checkpoints/lstm-features.048-0.225.hdf5\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 4s 250ms/step - loss: 0.3372 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2195 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.22509 to 0.21947, saving model to data/checkpoints/lstm-features.049-0.219.hdf5\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.3927 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2139 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.21947 to 0.21391, saving model to data/checkpoints/lstm-features.050-0.214.hdf5\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 5s 319ms/step - loss: 0.2479 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.21391 to 0.20848, saving model to data/checkpoints/lstm-features.051-0.208.hdf5\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3740 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2030 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.20848 to 0.20302, saving model to data/checkpoints/lstm-features.052-0.203.hdf5\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 5s 290ms/step - loss: 0.3188 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.20302 to 0.19779, saving model to data/checkpoints/lstm-features.053-0.198.hdf5\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 5s 286ms/step - loss: 0.3995 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.19779 to 0.19271, saving model to data/checkpoints/lstm-features.054-0.193.hdf5\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.4012 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.19271 to 0.18782, saving model to data/checkpoints/lstm-features.055-0.188.hdf5\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 6s 355ms/step - loss: 0.3151 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1829 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.18782 to 0.18289, saving model to data/checkpoints/lstm-features.056-0.183.hdf5\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 5s 295ms/step - loss: 0.4047 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1780 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.18289 to 0.17802, saving model to data/checkpoints/lstm-features.057-0.178.hdf5\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.3843 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.17802 to 0.17316, saving model to data/checkpoints/lstm-features.058-0.173.hdf5\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.3776 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1685 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.17316 to 0.16849, saving model to data/checkpoints/lstm-features.059-0.168.hdf5\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.2844 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.16849 to 0.16405, saving model to data/checkpoints/lstm-features.060-0.164.hdf5\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.3339 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.16405 to 0.15990, saving model to data/checkpoints/lstm-features.061-0.160.hdf5\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 5s 265ms/step - loss: 0.2875 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.15990 to 0.15605, saving model to data/checkpoints/lstm-features.062-0.156.hdf5\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 4s 264ms/step - loss: 0.3686 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1523 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.15605 to 0.15229, saving model to data/checkpoints/lstm-features.063-0.152.hdf5\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 4s 264ms/step - loss: 0.2198 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1488 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.15229 to 0.14880, saving model to data/checkpoints/lstm-features.064-0.149.hdf5\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.3336 - accuracy: 0.7647 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1455 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.14880 to 0.14554, saving model to data/checkpoints/lstm-features.065-0.146.hdf5\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 4s 262ms/step - loss: 0.3900 - accuracy: 0.8235 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.14554 to 0.14254, saving model to data/checkpoints/lstm-features.066-0.143.hdf5\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 5s 285ms/step - loss: 0.3350 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1390 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.14254 to 0.13901, saving model to data/checkpoints/lstm-features.067-0.139.hdf5\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 5s 271ms/step - loss: 0.3123 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1349 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.13901 to 0.13489, saving model to data/checkpoints/lstm-features.068-0.135.hdf5\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 4s 263ms/step - loss: 0.2789 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.13489 to 0.13058, saving model to data/checkpoints/lstm-features.069-0.131.hdf5\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 5s 271ms/step - loss: 0.2797 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.13058 to 0.12584, saving model to data/checkpoints/lstm-features.070-0.126.hdf5\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 5s 275ms/step - loss: 0.3429 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1217 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.12584 to 0.12173, saving model to data/checkpoints/lstm-features.071-0.122.hdf5\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 5s 302ms/step - loss: 0.2278 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.12173 to 0.11802, saving model to data/checkpoints/lstm-features.072-0.118.hdf5\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 5s 279ms/step - loss: 0.3213 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.11802 to 0.11426, saving model to data/checkpoints/lstm-features.073-0.114.hdf5\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 5s 304ms/step - loss: 0.2556 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1104 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.11426 to 0.11045, saving model to data/checkpoints/lstm-features.074-0.110.hdf5\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.2084 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.11045 to 0.10687, saving model to data/checkpoints/lstm-features.075-0.107.hdf5\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.2367 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.10687 to 0.10342, saving model to data/checkpoints/lstm-features.076-0.103.hdf5\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 5s 310ms/step - loss: 0.3488 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1002 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.10342 to 0.10024, saving model to data/checkpoints/lstm-features.077-0.100.hdf5\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 5s 297ms/step - loss: 0.2047 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.10024 to 0.09740, saving model to data/checkpoints/lstm-features.078-0.097.hdf5\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 5s 290ms/step - loss: 0.2745 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0947 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.09740 to 0.09468, saving model to data/checkpoints/lstm-features.079-0.095.hdf5\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 5s 271ms/step - loss: 0.2550 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0919 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.09468 to 0.09191, saving model to data/checkpoints/lstm-features.080-0.092.hdf5\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 5s 273ms/step - loss: 0.1863 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.09191 to 0.08920, saving model to data/checkpoints/lstm-features.081-0.089.hdf5\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 4s 250ms/step - loss: 0.2253 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0866 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.08920 to 0.08665, saving model to data/checkpoints/lstm-features.082-0.087.hdf5\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 5s 276ms/step - loss: 0.2945 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0841 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.08665 to 0.08411, saving model to data/checkpoints/lstm-features.083-0.084.hdf5\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 4s 264ms/step - loss: 0.2105 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.08411 to 0.08165, saving model to data/checkpoints/lstm-features.084-0.082.hdf5\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 5s 271ms/step - loss: 0.1740 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.08165 to 0.07930, saving model to data/checkpoints/lstm-features.085-0.079.hdf5\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 4s 257ms/step - loss: 0.2032 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.07930 to 0.07700, saving model to data/checkpoints/lstm-features.086-0.077.hdf5\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 4s 259ms/step - loss: 0.2106 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.07700 to 0.07479, saving model to data/checkpoints/lstm-features.087-0.075.hdf5\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 5s 267ms/step - loss: 0.2029 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.07479 to 0.07260, saving model to data/checkpoints/lstm-features.088-0.073.hdf5\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 5s 269ms/step - loss: 0.2140 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.07260 to 0.07047, saving model to data/checkpoints/lstm-features.089-0.070.hdf5\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 4s 256ms/step - loss: 0.1932 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.07047 to 0.06829, saving model to data/checkpoints/lstm-features.090-0.068.hdf5\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 4s 263ms/step - loss: 0.1727 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.06829 to 0.06616, saving model to data/checkpoints/lstm-features.091-0.066.hdf5\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 5s 313ms/step - loss: 0.2625 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0641 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.06616 to 0.06414, saving model to data/checkpoints/lstm-features.092-0.064.hdf5\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 5s 273ms/step - loss: 0.1991 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.06414 to 0.06205, saving model to data/checkpoints/lstm-features.093-0.062.hdf5\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.1531 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.06205 to 0.06008, saving model to data/checkpoints/lstm-features.094-0.060.hdf5\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 5s 270ms/step - loss: 0.2547 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss improved from 0.06008 to 0.05816, saving model to data/checkpoints/lstm-features.095-0.058.hdf5\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 5s 268ms/step - loss: 0.2007 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.05816 to 0.05632, saving model to data/checkpoints/lstm-features.096-0.056.hdf5\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 5s 273ms/step - loss: 0.1912 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.05632 to 0.05442, saving model to data/checkpoints/lstm-features.097-0.054.hdf5\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 5s 270ms/step - loss: 0.2136 - accuracy: 0.8824 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.05442 to 0.05258, saving model to data/checkpoints/lstm-features.098-0.053.hdf5\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 5s 283ms/step - loss: 0.2018 - accuracy: 0.9412 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.05258 to 0.05080, saving model to data/checkpoints/lstm-features.099-0.051.hdf5\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.1696 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.05080 to 0.04903, saving model to data/checkpoints/lstm-features.100-0.049.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0xd696cef60>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout, ZeroPadding3D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import sys\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "import time\n",
    "import os.path\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join('data', 'checkpoints','lstm-features' + '.{epoch:03d}-{val_loss:.3f}.hdf5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('data', 'logs', 'lstm'))\n",
    "\n",
    "# Helper: Stop when we stop learning.\n",
    "early_stopper = EarlyStopping(patience=10)\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('data', 'logs', 'lstm' + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "# Get the data and process it.\n",
    "data = DataSet(\n",
    "    seq_length=40,\n",
    "    class_limit=70\n",
    ")\n",
    "#listt=[]\n",
    "#listt2=[]\n",
    "X, y = data.get_all_sequences_in_memory('train', 'features')\n",
    "X_test, y_test = data.get_all_sequences_in_memory('test', 'features')\n",
    "# for i in range(len(X)):\n",
    "#  for j in range(70):\n",
    "  #   if (y[i][j]==1) and not(j in listt):\n",
    "  #    listt.append(j)\n",
    "# for i in range(len(X_test)):\n",
    "#  for j in range(70):\n",
    "  #   if (y_test[i][j]==1) and not(j in listt2):\n",
    "  #    listt2.append(j)\n",
    "#print(listt)\n",
    "#print(listt2)\n",
    "#listt3= []\n",
    "# for i in range(len(listt2)):\n",
    "#  if not(listt2[i] in listt):\n",
    "  #   listt3.append(listt2[i])\n",
    "#X_test2 = X_test.copy()\n",
    "#y_test2 = y_test.copy()\n",
    "#for i in range(len(X_test)):\n",
    "  # flag=1\n",
    "  #for j in range(70):\n",
    "    # if(y_test[i][j]==1) and (j in listt3):\n",
    "    #  flag=0\n",
    "      # break\n",
    "  #if flag==1:\n",
    "    # X_test2 = np.append(X_test2,[X_test[i]],axis=0)\n",
    "    #y_test2 = np.append(y_test2,[y_test[i]],axis=0)\n",
    "#print(X_test2.shape)\n",
    "#print(y_test2.shape)\n",
    "#l = X_test2.shape[0]-X_test.shape[0]\n",
    "#X_test = X_test2[-l:,:,:]\n",
    "#y_test = y_test2[-l:,:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(2048, return_sequences=False,input_shape=(40,2048),dropout=0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(data.classes), activation='softmax'))\n",
    "optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                    metrics=['accuracy','top_k_categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    callbacks=[tb, early_stopper, csv_logger,checkpointer],\n",
    "    epochs=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "1YyPmds-3oYO",
    "outputId": "8b9e240e-185e-4301-c543-022139447378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.avi  -------  Help\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from keras.preprocessing import image as Img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "import glob\n",
    "\n",
    "def rescale_list(input_list, size):\n",
    "    assert len(input_list) >= size\n",
    "    skip = len(input_list) // size\n",
    "    output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "    return output[:size]\n",
    "classes = glob.glob(\"data/train/*\")\n",
    "classes = [classes[i].split('/')[2] for i in range(len(classes))]\n",
    "classes = sorted(classes)\n",
    "\n",
    "import cv2 \n",
    "import os \n",
    "image_name = '9.avi'\n",
    "cam = cv2.VideoCapture(image_name) \n",
    "currentframe = 0\n",
    "  \n",
    "frames=[]\n",
    "while(True): \n",
    "    ret,frame = cam.read() \n",
    "    if ret: \n",
    "        # if video is still left continue creating images \n",
    "        name = 'testFinal/frame'+'9' +\"frame_no\"+ str(currentframe) + '.jpg'\n",
    "        cv2.imwrite(name, frame) \n",
    "        frames.append(name)  \n",
    "        currentframe += 1\n",
    "    else: \n",
    "        break\n",
    "cam.release() \n",
    "cv2.destroyAllWindows()\n",
    "rescaled_list = rescale_list(frames,40)\n",
    "\n",
    "base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=True\n",
    ")\n",
    "# We'll extract features at the final pool layer.\n",
    "inception_model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=base_model.get_layer('avg_pool').output\n",
    ")\n",
    "sequence = []\n",
    "for image in rescaled_list:\n",
    "        img = Img.load_img(image, target_size=(299, 299))\n",
    "        x = Img.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        features = inception_model.predict(x)\n",
    "        sequence.append(features[0])\n",
    "\n",
    "sequence = np.array([sequence])\n",
    "prediction = model.predict(sequence)\n",
    "maxm = prediction[0][0]\n",
    "maxid = 0\n",
    "for i in range(len(prediction[0])):\n",
    "      if(maxm<prediction[0][i]):\n",
    "            maxm = prediction[0][i]\n",
    "            maxid = i\n",
    "#print(frames)\n",
    "print(image_name,' ------- ',classes[maxid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8C1ZBYWsleEJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinalCVProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
